{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be wrangled is from tweet archive of Twitter user WeRateDogs.The data contains of tweet data between November,2015 to August,2017. The Account basically comments on other peoples dogs with humorous comment about the dog.\n",
    "\n",
    "Due to the images we have on WeRateDogs Dataset we have another dataset that consists of image predicitions along side each tweet_id,image url and the image number.No major wrangling that will be done on it but provides additional information for the main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather Twitter archive csv file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the link porvided by udacity i download the file and added uploaded it in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather tweet image prediction tsv file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using python's request library i was able to programmatically download the file and have it locally in a file named **image_prediction.tsv**.Then on the file to my code as a dataframe with the help of python library called pandas  imported it as img_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gathering data from Twitter API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of tweet IDs in the Twitter i was able to access the entire data for every tweet through Twitter API and store it in a JSON data in a file called **tweet_json.txt** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This i did it in two ways visual assessment and programmatic assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once i opened the twitter-archive-enhanced.csv and image_prediction.tsv i was able to observe 2 quality and 2 tidiness issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quality: source column had unnecessary html tags\n",
    "- Quality: text column had untruncated data instead of displayble data\n",
    "- Tidiness:pupper,floofer,doggo amd puppo columns and to be merged to one column name stage\n",
    "- Tidiness: duplicates in twitter archive data for columns with retweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**programmatic Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of Python library pandas i was able to carry out various methods that enabled i identify quality issues example methods i used include value_counts , info etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having records with more than one do stage\n",
    "- Dog names starting with lowercase characters\n",
    "- wrong data types used for certain columns such as timestamp column\n",
    "- Tweet_ids missing in the image prediction (img_df) dataframe that are in twitter archive \n",
    "- rating_denomitor coloumn as values more than 10\n",
    "- dataframe containing retweets hence having duplictaes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I carried out cleaning of dataframe df_1 by dealing with all the quality and tidiness issues i had mentioned,before that i created a copy of the table df_1 and had it named as archive_clean.I dealt with the quality and tidiness issues in 3 stages-defined the problem ,typed out the code, tested the code if it was sucessfull hence having it easier to observe each step that took place in fixing the issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After carring out the cleaning phase i stored the archive_clean dataframe in **twitter_archive_master.csv** file thats was autoproduced and stored as part of part of the project folders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
